# import os
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from PIL import Image
# import numpy as np
# import cv2
# import base64
# from torchvision import transforms, models
# from openai import OpenAI

# # ğŸ”¥ å…³é”®æ–°å¢ï¼šå¼•å…¥ open_clip åº“æ¥å¼¥è¡¥ç¼ºå¤±çš„è¯­ä¹‰åˆ†æ”¯
# import open_clip 

# # ==========================================
# # 1. å·¦æ‰‹ï¼šä½ çš„å¤–å–æ£€æµ‹ä¸“å®¶ (SRMResNet)
# #    (è´Ÿè´£ï¼šåˆ¤å®šçœŸå‡ + ç”Ÿæˆçƒ­åŠ›å›¾)
# # ==========================================
# class SRMConv2d(nn.Module):
#     def __init__(self, inc=3, outc=30):
#         super(SRMConv2d, self).__init__()
#         self.hpf = nn.Conv2d(inc, outc, kernel_size=5, padding=2, bias=False)
#     def forward(self, x): return self.hpf(x)

# class SRMResNet(nn.Module):
#     def __init__(self):
#         super(SRMResNet, self).__init__()
#         self.hpf = SRMConv2d(3, 30)
#         self.model_min = models.resnet50(weights=None)
#         self.model_min.conv1 = nn.Conv2d(30, 64, kernel_size=7, stride=2, padding=3, bias=False)
#         self.model_min.fc = nn.Linear(2048, 2) 
#     def forward(self, x):
#         x = self.hpf(x)
#         x = self.model_min(x)
#         return x

# # ==========================================
# # 2. å³æ‰‹ï¼šè¯­ä¹‰ç‰¹å¾æå–å™¨ (OpenCLIP)
# #    (è´Ÿè´£ï¼šæå–ç‰¹å¾ + æ•°æ®åº“æ£€ç´¢)
# #    æˆ‘ä»¬ç›´æ¥åŠ è½½å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸éœ€è¦ä½ è‡ªå·±ç»ƒï¼
# # ==========================================
# class SemanticExtractor:
#     def __init__(self, device):
#         print("ğŸŒ Initializing Semantic Extractor (OpenCLIP)...")
#         # ä½¿ç”¨ ViT-B-32ï¼Œå®ƒæ˜¯æœ€å¸¸ç”¨çš„ CLIP æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½
#         # ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æƒé‡ (çº¦300MB)
#         self.model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
#         self.model.to(device)
#         self.model.eval()
#         self.device = device

#     def extract(self, image_path):
#         """è¯»å–å›¾ç‰‡å¹¶æå– 512 ç»´è¯­ä¹‰å‘é‡"""
#         image = Image.open(image_path).convert('RGB')
#         # ä½¿ç”¨ CLIP è‡ªå·±çš„é¢„å¤„ç†
#         image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
#         with torch.no_grad():
#             features = self.model.encode_image(image_tensor)
#             features = F.normalize(features, dim=-1) # å½’ä¸€åŒ–
#         return features

# # ==========================================
# # 3. å¤§è„‘ï¼šGPT-4o æ³•åŒ»åˆ†æå¸ˆ
# #    (è´Ÿè´£ï¼šçœ‹å›¾ + çœ‹çƒ­åŠ›å›¾ + çœ‹æ£€ç´¢ç»“æœ -> å†™æŠ¥å‘Š)
# # ==========================================
# class ForensicBrain:
#     def __init__(self, api_key):
#         self.client = OpenAI(api_key=api_key, base_url="https://api.openai.com/v1")

#     def encode_image(self, image_path):
#         if not os.path.exists(image_path): return None
#         with open(image_path, "rb") as image_file:
#             return base64.b64encode(image_file.read()).decode('utf-8')

#     def generate_report(self, case_data):
#         base64_origin = self.encode_image(case_data['original_img_path'])
#         base64_cam = self.encode_image(case_data['cam_img_path'])
        
#         system_prompt = """
#         You are an AI Forensic Expert. Combine "Pixel Artifacts" and "Semantic Logic" to verify images.
        
#         Your Analysis Process:
#         1. **Artifacts (Hard Evidence):** Check the Heatmap. Red areas = algorithm traces detected by SRM-ResNet.
#         2. **Semantics (Knowledge):** The system has retrieved similar known AI cases from the database based on semantic features.
#         3. **Visual Logic:** Check the Original Image for physical/lighting flaws.
        
#         Output a professional report explaining WHY it is Fake.
#         """
        
#         user_prompt = f"""
#         [Case Data]
#         - Detection Verdict: {case_data['aide_verdict']} (Confidence: {case_data['aide_conf']:.2%})
#         - Semantic Retrieval: The image is semantically similar to these known generation patterns:
#           {case_data['similar_evidence']}
        
#         Please analyze the Original Image and the Artifact Heatmap.
#         """
        
#         messages = [
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": [
#                 {"type": "text", "text": user_prompt},
#                 {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_origin}"}},
#                 {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_cam}"} if base64_cam else {"type": "text", "text": "No heatmap."}}
#             ]}
#         ]
        
#         print("ğŸ¤– Calling GPT-4o...")
#         try:
#             response = self.client.chat.completions.create(
#                 model="gpt-4o", messages=messages, temperature=0.3
#             )
#             return response.choices[0].message.content
#         except Exception as e:
#             return f"GPT Error: {e}"

# # ==========================================
# # 4. æ€»æŒ‡æŒ¥ï¼šå¯è§£é‡Šæ€§æ£€æµ‹ç³»ç»Ÿ
# # ==========================================
# class ExplainableDetector:
#     def __init__(self, model_path, db_path, api_key, patch_dir, device='cpu'):
#         self.device = torch.device(device)
#         print(f"ğŸš€ Initializing System on {self.device}...")
        
#         # 1. ä½ çš„æ£€æµ‹æ¨¡å‹ (ä¿æŒä¸å˜)
#         self.detector = SRMResNet()
#         self._load_detector_weights(model_path)
#         self.detector.to(self.device)
#         self.detector.eval()
        
#         # 2. è¯­ä¹‰æå– (ä¿æŒä¸å˜)
#         self.semantic_extractor = SemanticExtractor(self.device)
        
#         # 3. GPT å¤§è„‘ (ä¿æŒä¸å˜)
#         self.brain = ForensicBrain(api_key)
        
#         # 4. æ•°æ®åº“ (ä¿æŒä¸å˜)
#         self.vector_db = None
#         if os.path.exists(db_path):
#             self.vector_db = torch.load(db_path, map_location=self.device)
            
#         # 5. [æ–°å¢] ç»„é•¿çš„ç‰¹å¾å›¾æ–‡ä»¶å¤¹
#         self.patch_dir = patch_dir
#         if not os.path.exists(self.patch_dir):
#             print(f"âš ï¸ Warning: Patch directory not found: {self.patch_dir}")

#         self.trans = transforms.Compose([
#             transforms.Resize([256, 256]),
#             transforms.CenterCrop(224),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
#         ])

#     def _load_detector_weights(self, path):
#         # ... (ä¿æŒä¸å˜) ...
#         if not os.path.exists(path): raise FileNotFoundError(f"Model not found: {path}")
#         ckpt = torch.load(path, map_location='cpu')
#         state = ckpt['model'] if 'model' in ckpt else ckpt
#         new_state = {k.replace("module.", ""): v for k, v in state.items()}
#         self.detector.load_state_dict(new_state, strict=False) 
#         print("âœ… Food Detector weights loaded.")

#     def search_db(self, query_feat):
#         # ... (ä¿æŒä¸å˜) ...
#         if not self.vector_db: return "Database not available."
#         query_feat = query_feat.detach().cpu()
#         sims = []
#         for name, db_feat in self.vector_db.items():
#             db_feat = db_feat.cpu()
#             if db_feat.shape[-1] != query_feat.shape[-1]: continue 
#             sim = F.cosine_similarity(query_feat, db_feat).item()
#             sims.append((name, sim))
#         top3 = sorted(sims, key=lambda x: x[1], reverse=True)[:3]
#         return ", ".join([f"{n} ({s:.2f})" for n, s in top3])

#     def get_patch_image_path(self, original_image_path):
#         """
#         æ ¹æ®åŸå›¾è·¯å¾„ï¼Œå»ç»„é•¿çš„æ–‡ä»¶å¤¹é‡Œæ‰¾å¯¹åº”çš„ patch_all å›¾ç‰‡
#         å‡è®¾è§„åˆ™ï¼šåŸå›¾ xxx.jpg -> ç»„é•¿å›¾ patch_all_xxx.jpg
#         """
#         filename = os.path.basename(original_image_path)
#         # âš ï¸ è¯·æ ¹æ®ç»„é•¿çš„å®é™…å‘½åè§„åˆ™ä¿®æ”¹è¿™é‡Œ
#         patch_filename = f"patch_all_{filename}" 
#         # æˆ–è€…å¦‚æœç»„é•¿åªæ˜¯ä¿æŒåŸåï¼š patch_filename = filename
        
#         full_path = os.path.join(self.patch_dir, patch_filename)
#         if os.path.exists(full_path):
#             return full_path
#         else:
#             print(f"âš ï¸ æ²¡æ‰¾åˆ°å¯¹åº”çš„ç‰¹å¾å›¾: {full_path}")
#             return None

#     def run(self, image_path):
#         print(f"\nğŸ” Processing: {image_path}")
        
#         # 1. è§†è§‰æ£€æµ‹ (Run Your Model)
#         img_pil = Image.open(image_path).convert('RGB')
#         input_tensor = self.trans(img_pil).unsqueeze(0).to(self.device)
        
#         self.detector.zero_grad()
#         logits = self.detector(input_tensor)
#         probs = F.softmax(logits, dim=1)
#         pred_idx = torch.argmax(probs, dim=1).item()
#         conf = probs[0][pred_idx].item()
        
#         # å‡è®¾ 0=Fake
#         verdict = "FAKE" if pred_idx == 0 else "REAL"
#         print(f"   ğŸ‘‰ Verdict: {verdict} ({conf:.2%})")

#         # 2. è·å–è¯æ®å›¾ (ä½¿ç”¨ç»„é•¿çš„å›¾ï¼)
#         evidence_path = self.get_patch_image_path(image_path)
        
#         # å¦‚æœæ‰¾ä¸åˆ°ç»„é•¿çš„å›¾ï¼Œå°±åªç”¨åŸå›¾ï¼Œæˆ–è€…ä½ å¯ä»¥ä¿ç•™ä¹‹å‰çš„ Heatmap é€»è¾‘ä½œä¸ºå¤‡é€‰
#         if evidence_path:
#             print(f"   ğŸ“¸ Found AIDE Feature Map: {os.path.basename(evidence_path)}")
#         else:
#             print("   âš ï¸ Using Original Image only (Feature map missing).")
#             evidence_path = None

#         # 3. è¯­ä¹‰æ£€ç´¢ (ä¿æŒä¸å˜)
#         semantic_feat = self.semantic_extractor.extract(image_path)
#         sim_result = self.search_db(semantic_feat)

#         # 4. GPT-4o ç”ŸæˆæŠ¥å‘Š (Prompt éœ€è¦å¾®è°ƒä»¥é€‚åº”æ–°å›¾)
#         # æˆ‘ä»¬éœ€è¦åœ¨ prompt é‡Œè§£é‡Š Max/Min çš„å«ä¹‰
#         self.brain_generate_report_v2(image_path, evidence_path, verdict, conf, sim_result)

#     def brain_generate_report_v2(self, original_path, evidence_path, verdict, conf, sim_result):
#         # ä¸“é—¨é’ˆå¯¹ AIDE ç‰¹å¾å›¾çš„ Prompt
#         system_prompt = """
#         You are an AI Forensic Expert. You analyze images using "Frequency Domain Artifacts" (AIDE method).
        
#         Input Images:
#         1. Original Image.
#         2. **Artifact Feature Map**: This image highlights specific patches based on frequency analysis:
#            - **Max Regions (High Frequency):** Look for unnatural sharp edges, noise patterns, or "jagged" artifacts.
#            - **Min Regions (Low Frequency):** Look for unnatural smoothness, blurring, or texture loss.
        
#         Task:
#         - Analyze the "Feature Map" to identify where the algorithm detected anomalies.
#         - Combine this with the "Detection Verdict" and "Semantic Retrieval" results.
#         - Explain WHY the image is Real or Fake based on these specific regions.
#         """
        
#         user_prompt = f"""
#         [Case Data]
#         - Verdict: {verdict} (Confidence: {conf:.2%})
#         - Semantic Retrieval: Similar to {sim_result}
        
#         Please analyze the Original Image and the provided Feature Map. Focus on the Max/Min artifact regions.
#         """
        
#         # è°ƒç”¨ GPT (å¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œåªæ˜¯æ¢äº† prompt)
#         # è¿™é‡Œä¸ºäº†ç®€æ´ç›´æ¥æŠŠ brain çš„é€»è¾‘æ¬è¿‡æ¥æˆ–è€…è°ƒç”¨ brain çš„æ–¹æ³•
#         # å‡è®¾ä½ è¿˜åœ¨ç”¨ä¹‹å‰çš„ ForensicBrain ç±»ï¼Œè¿™é‡Œç¨ä½œé€‚é…
#         base64_origin = self.brain.encode_image(original_path)
#         base64_evidence = self.brain.encode_image(evidence_path) if evidence_path else None
        
#         messages = [
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": [
#                 {"type": "text", "text": user_prompt},
#                 {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_origin}"}},
#             ]}
#         ]
        
#         if base64_evidence:
#             messages[0]["content"] += "\n(The second image is the Artifact Feature Map)"
#             messages[1]["content"].append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_evidence}"}})
            
#         print("ğŸ¤– Calling GPT-4o...")
#         try:
#             response = self.brain.client.chat.completions.create(
#                 model="gpt-4o", messages=messages, temperature=0.3
#             )
#             print("\nğŸ“ FINAL REPORT:\n" + response.choices[0].message.content)
#         except Exception as e:
#             print(f"GPT Error: {e}")
            
# if __name__ == "__main__":
#     # é…ç½®
#     API_KEY = "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" # æ›¿æ¢ä½ çš„ Key
#     CKPT = r"D:\åŒ—å¤§\æ·±åº¦å­¦ä¹ \å¤§ä½œä¸š\Final_Augmented_Food_Detector.pth"
#     DB = "fake_image_vectors.pt"
    
#     # å›¾ç‰‡è·¯å¾„
#     IMG = r"D:\åŒ—å¤§\æ·±åº¦å­¦ä¹ \å¤§ä½œä¸š\AIå¤–å–æµ‹è¯•\jimeng-2025-12-19-9443-è¿‡æœŸçš„ç½å¤´é£Ÿå“ï¼Œç½å¤´è¡¨é¢æœ‰é”ˆè¿¹ï¼Œæ ‡ç­¾å‘é»„è¤ªè‰²ï¼Œæœ‰äº›ç½å¤´ç›–å­é¼“èµ·å˜å½¢ï¼Œè‡ªç„¶å†™å®æ‘„å½±....jpg"
    
#     if os.path.exists(IMG):
#         # åªè¦ä½ æœ‰ GPU é©±åŠ¨ï¼Œè¿™é‡Œå¯ä»¥ç”¨ 'cuda'ï¼Œå¦åˆ™ 'cpu'
#         detector = ExplainableDetector(CKPT, DB, API_KEY, device='cpu')
#         detector.run(IMG)
#     else:
#         print("Image not found!")
import os
import shutil
import subprocess
import torch
import torch.nn.functional as F
from PIL import Image
import base64
from torchvision import transforms, models
from openai import OpenAI
import open_clip
import glob
import time

# ==========================================
# 1. è¾…åŠ©å‡½æ•°ï¼šè‡ªåŠ¨åŒ–è¿è¡Œç»„é•¿çš„ Eval æµç¨‹
# ==========================================
def run_leader_eval_process(original_image_path, project_root, model_ckpt):
    """
    å…¨è‡ªåŠ¨æµæ°´çº¿ï¼š
    1. å»ºæ–‡ä»¶å¤¹ -> 2. ç§»å›¾ç‰‡ -> 3. è·‘å‘½ä»¤ -> 4. æ‹¿ç»“æœ
    """
    print(f"ğŸ­ [Pipeline] Starting AIDE Feature Generation for: {os.path.basename(original_image_path)}")
    
    # --- A. å‡†å¤‡æ‰€æœ‰çš„è·¯å¾„ ---
    # ç»„é•¿è¦æ±‚çš„ç»“æ„: Data/eval/test_set/1_fake/å›¾ç‰‡
    eval_dir = os.path.join(project_root, "Data", "eval", "test_set", "1_fake")
    train_dir = os.path.join(project_root, "Data", "train", "dummy", "1_fake") # éª—è¿‡bugç”¨çš„
    output_dir = os.path.join(project_root, "bowen_results") # ç»“æœå­˜è¿™é‡Œ
    
    # æ¸…ç†å¹¶é‡å»ºä¸´æ—¶ç›®å½• (ä¿è¯æ¯æ¬¡åªè·‘è¿™ä¸€å¼ å›¾)
    if os.path.exists(os.path.join(project_root, "Data")):
        shutil.rmtree(os.path.join(project_root, "Data")) # æš´åŠ›æ¸…ç†æ—§æ•°æ®
    
    os.makedirs(eval_dir, exist_ok=True)
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    # --- B. æŠŠå›¾ç‰‡å¤åˆ¶è¿›å» ---
    target_path = os.path.join(eval_dir, os.path.basename(original_image_path))
    shutil.copy(original_image_path, target_path)
    
    # --- C. æ‹¼å‡‘è¿è¡Œå‘½ä»¤ ---
    # è¿™å°±æ˜¯æŠŠç»„é•¿çš„ eval.sh ç¿»è¯‘æˆ Python è°ƒç”¨
    # æ³¨æ„ï¼šæˆ‘ä»¬å¿…é¡»åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œå¦åˆ™æ‰¾ä¸åˆ° main_finetune.py
    cmd = [
        "python", "-m", "torch.distributed.launch",
        "--nproc_per_node", "1",
        "--master_port", "29505", # æ¢ä¸ªç«¯å£é˜²æ­¢å†²çª
        "main_finetune.py",
        "--model", "AIDE",
        "--batch_size", "1",
        "--blr", "5e-4",
        "--epochs", "1", # è·‘1è½®å°±å¤Ÿäº†
        "--data_path", "Data/train",      # ä¼ ä¸ªç©ºè·¯å¾„éª—è¿‡å®ƒ
        "--eval_data_path", "Data/eval",  # çœŸå®çš„å›¾ç‰‡åœ¨è¿™é‡Œ
        "--resume", model_ckpt,           # ä½ çš„æƒé‡
        "--eval", "True",                 # å¼€å¯è¯„æµ‹æ¨¡å¼
        "--output_dir", "bowen_results"   # è¾“å‡ºè·¯å¾„
    ]
    
    print("   âš™ï¸ Running AIDE Eval Command (This may take a few seconds)...")
    try:
        # æ‰§è¡Œå‘½ä»¤ (cwd=project_root ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ)
        subprocess.run(cmd, cwd=project_root, check=True, shell=True)
    except subprocess.CalledProcessError as e:
        print(f"   âŒ Eval command failed: {e}")
        return None

    # --- D. æ‰¾ç”Ÿæˆçš„å›¾ ---
    # ç»„é•¿ç”Ÿæˆçš„å›¾é€šå¸¸åœ¨ output_dir ä¸‹é¢ï¼Œæˆ–è€…æ˜¯ output_dir/patch_all_xxx.png
    # æˆ‘ä»¬éå†æ‰¾ä¸€ä¸‹
    patch_name = f"patch_all_{os.path.basename(original_image_path)}"
    # æœ‰æ—¶å€™å®ƒæ˜¯ pngï¼Œæœ‰æ—¶å€™ä¿ç•™åŸåç¼€ï¼Œæˆ‘ä»¬è¦çµæ´»ä¸€ç‚¹
    search_pattern = os.path.join(output_dir, "patch_all_*")
    found_files = glob.glob(search_pattern)
    
    # ç®€å•çš„åŒ¹é…é€»è¾‘ï¼šæ‰¾æœ€æ–°çš„é‚£å¼ ï¼Œæˆ–è€…åå­—åŒ¹é…çš„
    if found_files:
        # å‡è®¾å°±æ˜¯æœ€æ–°ç”Ÿæˆçš„é‚£å¼ 
        latest_file = max(found_files, key=os.path.getctime)
        print(f"   ğŸ“¸ Feature Map Generated: {latest_file}")
        return latest_file
    
    print("   âš ï¸ No output image found.")
    return None

# ==========================================
# 2. ä½ çš„æ£€æµ‹æ¨¡å‹ (SRMResNet) - è´Ÿè´£æ‰“åˆ†
# ==========================================
class SRMConv2d(torch.nn.Module):
    def __init__(self, inc=3, outc=30):
        super(SRMConv2d, self).__init__()
        self.hpf = torch.nn.Conv2d(inc, outc, kernel_size=5, padding=2, bias=False)
    def forward(self, x): return self.hpf(x)

class SRMResNet(torch.nn.Module):
    def __init__(self):
        super(SRMResNet, self).__init__()
        self.hpf = SRMConv2d(3, 30)
        self.model_min = models.resnet50(weights=None)
        self.model_min.conv1 = torch.nn.Conv2d(30, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.model_min.fc = torch.nn.Linear(2048, 2) 
    def forward(self, x):
        x = self.hpf(x)
        x = self.model_min(x)
        return x

# ==========================================
# 3. è¯­ä¹‰ç‰¹å¾æå– (OpenCLIP) - è´Ÿè´£æ‰¾åŒä¼™
# ==========================================
class SemanticExtractor:
    def __init__(self, device):
        print("ğŸŒ Initializing Semantic Extractor (OpenCLIP)...")
        self.model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
        self.model.to(device)
        self.model.eval()
        self.device = device

    def extract(self, image_path):
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            features = F.normalize(features, dim=-1)
        return features

# ==========================================
# 4. GPT-4o å¤§è„‘ - è´Ÿè´£å†™æŠ¥å‘Š
# ==========================================
class ForensicBrain:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key, base_url="https://api.openai.com/v1")

    def encode_image(self, image_path):
        if not image_path or not os.path.exists(image_path): return None
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def generate_report(self, case_data):
        base64_origin = self.encode_image(case_data['original_img_path'])
        base64_evidence = self.encode_image(case_data['evidence_path'])
        
        system_prompt = """
        You are an AI Forensic Expert using the AIDE framework.
        Interpreting the AIDE Feature Map (Image 2):
        - **Red/Blue Dots (Max Regions):** High-Frequency anomalies (sharp edges, noise).
        - **Green/Yellow Dots (Min Regions):** Low-Frequency anomalies (unnatural smoothness).
        
        Task: Write a forensic report. combine the Detector Verdict, Semantic Retrieval, and visual analysis of the dots.
        """
        
        user_prompt = f"""
        [Case Data]
        - Verdict: {case_data['aide_verdict']} ({case_data['aide_conf']:.2%})
        - Semantic Retrieval: {case_data['similar_evidence']}
        
        Analyze the Feature Map (Image 2). Explain why the model marked those specific spots.
        """
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": [
                {"type": "text", "text": user_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_origin}"}},
            ]}
        ]
        
        if base64_evidence:
            messages[1]["content"].append(
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_evidence}"}}
            )
            messages[1]["content"].append({"type": "text", "text": "(This is the AIDE Feature Map)"})
        else:
            messages[1]["content"].append({"type": "text", "text": "(Feature Map missing, analyze original only)"})

        print("ğŸ¤– Calling GPT-4o...")
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o", messages=messages, temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"GPT Error: {e}"

# ==========================================
# 5. æ€»æŒ‡æŒ¥
# ==========================================
class ExplainableDetector:
    def __init__(self, model_path, db_path, api_key, project_root, device='cpu'):
        self.device = torch.device(device)
        self.project_root = project_root
        self.model_path = model_path # ä¿å­˜ä¸‹æ¥ç»™ subprocess ç”¨
        print(f"ğŸš€ Initializing System on {self.device}...")
        
        self.detector = SRMResNet()
        self._load_detector_weights(model_path)
        self.detector.to(self.device)
        self.detector.eval()
        
        self.semantic_extractor = SemanticExtractor(self.device)
        self.brain = ForensicBrain(api_key)
        self.vector_db = torch.load(db_path, map_location=self.device) if os.path.exists(db_path) else None
        
        self.trans = transforms.Compose([
            transforms.Resize([256, 256]),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def _load_detector_weights(self, path):
        if not os.path.exists(path): raise FileNotFoundError(f"Model not found: {path}")
        ckpt = torch.load(path, map_location='cpu')
        state = ckpt['model'] if 'model' in ckpt else ckpt
        new_state = {k.replace("module.", ""): v for k, v in state.items()}
        self.detector.load_state_dict(new_state, strict=False) 
        print("âœ… Food Detector weights loaded.")

    def search_db(self, query_feat):
        if not self.vector_db: return "Database not available."
        query_feat = query_feat.detach().cpu()
        sims = []
        for name, db_feat in self.vector_db.items():
            db_feat = db_feat.cpu()
            if db_feat.shape[-1] != query_feat.shape[-1]: continue 
            sim = F.cosine_similarity(query_feat, db_feat).item()
            sims.append((name, sim))
        top3 = sorted(sims, key=lambda x: x[1], reverse=True)[:3]
        return ", ".join([f"{n} ({s:.2f})" for n, s in top3])

    def run(self, image_path):
        print(f"\nğŸ” Processing: {image_path}")
        
        # 1. ä½ çš„æ¨¡å‹æ£€æµ‹ (Hard Evidence)
        img_pil = Image.open(image_path).convert('RGB')
        input_tensor = self.trans(img_pil).unsqueeze(0).to(self.device)
        self.detector.zero_grad()
        logits = self.detector(input_tensor)
        probs = F.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        conf = probs[0][pred_idx].item()
        verdict = "FAKE" if pred_idx == 0 else "REAL" 
        print(f"   ğŸ‘‰ Verdict: {verdict} ({conf:.2%})")

        # 2. è°ƒç”¨ç»„é•¿çš„è„šæœ¬ç”Ÿæˆç‰¹å¾å›¾ (The "X-Ray")
        # è¿™ä¸€æ­¥æ˜¯å…¨è‡ªåŠ¨çš„ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨ç§»æ–‡ä»¶
        evidence_path = run_leader_eval_process(image_path, self.project_root, self.model_path)

        # 3. è¯­ä¹‰æ£€ç´¢ (Semantic Evidence)
        semantic_feat = self.semantic_extractor.extract(image_path)
        sim_result = self.search_db(semantic_feat)
        if sim_result: print(f"   ğŸ“š Retrieval: {sim_result[:50]}...")

        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.brain.generate_report({
            'original_img_path': image_path,
            'evidence_path': evidence_path, 
            'aide_verdict': verdict,
            'aide_conf': conf,
            'similar_evidence': sim_result
        })
        
        print("\nğŸ“ FINAL REPORT:\n" + report)

if __name__ == "__main__":
    # === å¿…å¡«é…ç½® ===
    API_KEY = "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"
    
    # ä½ çš„æ¨¡å‹è·¯å¾„
    CKPT = r"D:\åŒ—å¤§\æ·±åº¦å­¦ä¹ \å¤§ä½œä¸š\Final_Augmented_Food_Detector.pth"
    # æ•°æ®åº“è·¯å¾„
    DB = "fake_image_vectors.pt"
    # é¡¹ç›®æ ¹ç›®å½• (éå¸¸é‡è¦ï¼å°±æ˜¯å«æœ‰ main_finetune.py çš„é‚£ä¸ªæ–‡ä»¶å¤¹)
    PROJECT_ROOT = r"D:\åŒ—å¤§\æ·±åº¦å­¦ä¹ \å¤§ä½œä¸š\AIDE-main\AIDE-main"
    
    # ä½ è¦æµ‹è¯•çš„å›¾ç‰‡ (éšä¾¿æ”¾å“ªé‡Œéƒ½è¡Œï¼Œç¨‹åºä¼šè‡ªåŠ¨æŠŠå®ƒå¤åˆ¶åˆ°è¯¥å»çš„åœ°æ–¹)
    IMG = r"D:\åŒ—å¤§\æ·±åº¦å­¦ä¹ \å¤§ä½œä¸š\AIå¤–å–æµ‹è¯•\jimeng-2025-12-19-8882-æ´’å‡ºçš„ä¾¿å½“ç›’ï¼Œå¡‘æ–™ä¾¿å½“ç›’ç›–å­æ²¡æœ‰ç›–ç´§ï¼Œé‡Œé¢çš„é¥­èœæ´’å‡ºåˆ°ç›’å­å¤–é¢ï¼Œç±³é¥­å’Œé…èœæ•£è½ï¼Œ....jpg"
    
    if os.path.exists(IMG):
        # åˆå§‹åŒ– detector (æ³¨æ„ device='cpu' æ¯”è¾ƒç¨³å¦¥)
        detector = ExplainableDetector(CKPT, DB, API_KEY, PROJECT_ROOT, device='cpu')
        detector.run(IMG)
    else:
        print("Image not found!")