import os
import glob
import torch
import torch.nn.functional as F
from PIL import Image
import base64
from torchvision import transforms, models
from openai import OpenAI
import open_clip
from tqdm import tqdm
import time

# =================é…ç½®åŒºåŸŸ=================
# è¿™æ ·æ— è®ºåˆ«äººæŠŠæ–‡ä»¶å¤¹æ”¾åœ¨å“ªé‡Œï¼ŒBASE_DIR éƒ½ä¼šè‡ªåŠ¨å˜æˆé‚£ä¸ªè·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ğŸ”´ å¿…å¡«ï¼šä¸Šä¼  Github å‰ï¼Œè¯·åŠ¡å¿…åˆ é™¤çœŸå®çš„ Keyï¼Œæ¢æˆæç¤ºè¯­æˆ–ç¯å¢ƒå˜é‡ï¼
API_KEY = "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"  # ğŸ”´ å¿…å¡«ï¼šæ¢æˆä½ çš„ Key

# æ¨¡å‹æƒé‡ï¼šos.path.join ä¼šè‡ªåŠ¨æ‹¼æ¥è·¯å¾„ï¼Œå…¼å®¹ Windows å’Œ Mac
CKPT_PATH = os.path.join(BASE_DIR, "Final_Augmented_Food_Detector.pth")

# å‘é‡æ•°æ®åº“
DB_PATH = os.path.join(BASE_DIR, "fake_image_vectors.pt")

# ğŸ“‚ è¾“å…¥ï¼šåŸå§‹ AI å‡å›¾æ–‡ä»¶å¤¹
RAW_IMG_DIR = os.path.join(BASE_DIR, "AI_JPG")

# ğŸ“‚ è¾“å…¥ï¼šç»„é•¿ç”Ÿæˆçš„ç‰¹å¾å›¾æ–‡ä»¶å¤¹
PATCH_DIR = os.path.join(BASE_DIR, "eval_cam_results_db_bowen")

# ğŸ’¾ è¾“å‡ºï¼šæŠ¥å‘Šä¿å­˜è·¯å¾„
OUTPUT_FILE = os.path.join(BASE_DIR, "FINAL_FORENSIC_REPORTS.txt")

# ==========================================
# --- 1. å®šä¹‰æ£€æµ‹æ¨¡å‹ ---
class SRMConv2d(torch.nn.Module):
    def __init__(self, inc=3, outc=30):
        super(SRMConv2d, self).__init__()
        self.hpf = torch.nn.Conv2d(inc, outc, kernel_size=5, padding=2, bias=False)
    def forward(self, x): return self.hpf(x)

class SRMResNet(torch.nn.Module):
    def __init__(self):
        super(SRMResNet, self).__init__()
        self.hpf = SRMConv2d(3, 30)
        # weights=None è§£å†³ä½ ä¹‹å‰çš„ warning
        self.model_min = models.resnet50(weights=None)
        self.model_min.conv1 = torch.nn.Conv2d(30, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.model_min.fc = torch.nn.Linear(2048, 2) 
    def forward(self, x):
        x = self.hpf(x)
        x = self.model_min(x)
        return x

# --- 2. å®šä¹‰è¯­ä¹‰æå–å™¨ ---
class SemanticExtractor:
    def __init__(self, device):
        self.model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
        self.model.to(device)
        self.model.eval()
        self.device = device
    def extract(self, image_path):
        try:
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            with torch.no_grad():
                features = self.model.encode_image(image_tensor)
                features = F.normalize(features, dim=-1)
            return features
        except:
            return None

# --- 3. å®šä¹‰ GPT å¤§è„‘ ---
class ForensicBrain:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key, base_url="https://api.openai.com/v1")

    def encode_image(self, image_path):
        if not image_path or not os.path.exists(image_path): return None
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def analyze_case(self, case_data):
        base64_origin = self.encode_image(case_data['origin_path'])
        base64_patch = self.encode_image(case_data['patch_path'])
        
        import json
        
        # 1. å®šä¹‰ç³»ç»Ÿè§’è‰² (JSONç»“æ„)
        prompt_structure = {
            "role": "Senior Digital Forensic Expert (AIDE Framework)",
            "task": "Analyze visual evidence to determine authenticity. Do NOT rely on metadata.",
            "visual_evidence_legend": {
                "Red/Blue_Dots": "MAX Regions (High-Freq Artifacts): jagged edges, noise.",
                "Green/Yellow_Dots": "MIN Regions (Low-Freq Artifacts): waxy smoothness, texture loss."
            },
            "workflow": [
                "Step 1: Check Detector Confidence (Hard Evidence).",
                "Step 2: Analyze Feature Map (Microscopic Visual Artifacts).",
                "Step 3: Check Semantic Consistency (Database Matches).",
                "Step 4: Conclusion based on Physics & Logic."
            ]
        }
        system_content = json.dumps(prompt_structure, indent=2)

        # 2. å®šä¹‰ç”¨æˆ·è¾“å…¥ (æ³¨æ„ï¼šè¿™é‡Œä¸å†åŒ…å« filename)
        # å˜é‡åç»Ÿä¸€ä¸º user_content_strï¼Œè§£å†³ NameError
        user_content_str = f"""
        [EVIDENCE DATA]
        - Detector Verdict: {case_data['verdict']} (Confidence: {case_data['conf']:.2%})
        - Database Retrieval: {case_data['semantic_info']}
        
        [INSTRUCTION]
        Analyze the Original Image and the Feature Map (if available).
        Focus on:
        1. Physical inconsistencies (Light, Shadow, Gravity).
        2. Frequency artifacts marked by the dots.
        3. Do NOT make assumptions based on the filename (it is hidden).
        """
        
        # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": [
                {"type": "text", "text": user_content_str}, # ğŸ‘ˆ è¿™é‡Œç°åœ¨èƒ½æ‰¾åˆ°å˜é‡äº†
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_origin}"}},
            ]}
        ]
        
        if base64_patch:
            messages[1]["content"].append(
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_patch}"}}
            )
            messages[1]["content"].append({"type": "text", "text": "(Image 2: AIDE Feature Map)"})
        else:
            messages[1]["content"].append({"type": "text", "text": "(Image 2: Feature Map Missing, analyze original only)"})

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o", messages=messages, temperature=0.4
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"GPT Error: {e}"
        
# --- 4. ä¸»æµç¨‹ ---
def main():
    device = "cpu" # æ¨è CPU æ¯”è¾ƒç¨³
    print(f"ğŸš€ Starting Batch Forensic Analysis on {device}...")

    # A. åŠ è½½æ‰€æœ‰ç»„ä»¶
    detector = SRMResNet()
    ckpt = torch.load(CKPT_PATH, map_location='cpu')
    state = ckpt['model'] if 'model' in ckpt else ckpt
    detector.load_state_dict({k.replace("module.", ""): v for k, v in state.items()}, strict=False)
    detector.to(device).eval()
    print("âœ… Detector Loaded.")
    
    semantic_extractor = SemanticExtractor(device)
    print("âœ… Semantic Extractor Loaded.")
    
    brain = ForensicBrain(API_KEY)
    
    vector_db = None
    if os.path.exists(DB_PATH):
        vector_db = torch.load(DB_PATH, map_location=device)
        print("âœ… Vector DB Loaded.")
    
    # B. å‡†å¤‡æ•°æ®è½¬æ¢
    trans = transforms.Compose([
        transforms.Resize([256, 256]), transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # C. æ‰«ææ‰€æœ‰å¾…æµ‹å›¾ç‰‡
    image_files = glob.glob(os.path.join(RAW_IMG_DIR, "*.*"))
    # è¿‡æ»¤éå›¾ç‰‡æ–‡ä»¶
    image_files = [f for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    print(f"ğŸ“‚ Found {len(image_files)} images to process.")
    if len(image_files) == 0:
        print(f"âŒ Error: No images in {RAW_IMG_DIR}")
        return

    # æ¸…ç©ºæˆ–æ–°å»ºè¾“å‡ºæ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("=== AIDE FORENSIC BATCH REPORT ===\n\n")

    # D. å¼€å§‹å¾ªç¯å¤„ç†
    for img_path in tqdm(image_files, desc="Processing"):
        filename = os.path.basename(img_path)
        
        # 1. æ£€æµ‹ (Hard Evidence)
        try:
            img_pil = Image.open(img_path).convert('RGB')
            input_tensor = trans(img_pil).unsqueeze(0).to(device)
            with torch.no_grad():
                logits = detector(input_tensor)
                probs = F.softmax(logits, dim=1)
                pred_idx = torch.argmax(probs, dim=1).item()
                conf = probs[0][pred_idx].item()
                verdict = "FAKE" if pred_idx == 0 else "REAL"
        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue

        # 2. åŒ¹é…ç»„é•¿çš„ç‰¹å¾å›¾ (Visual Evidence)
        # è§„åˆ™ï¼šå» bowen æ–‡ä»¶å¤¹æ‰¾ patch_all_{name}.*
        name_no_ext = os.path.splitext(filename)[0]
        patch_path = None
        
        # å°è¯•å‡ ç§å¯èƒ½çš„å‘½å (jpg, png, æœ‰åç¼€, æ— åç¼€)
        candidates = [
            f"patch_all_{filename}",        # patch_all_name.jpg
            f"patch_all_{filename}.png",    # patch_all_name.jpg.png
            f"patch_all_{name_no_ext}.png", # patch_all_name.png
            f"patch_all_{name_no_ext}.jpg"  # patch_all_name.jpg
        ]
        
        for cand in candidates:
            full_path = os.path.join(PATCH_DIR, cand)
            if os.path.exists(full_path):
                patch_path = full_path
                break
        
        # 3. è¯­ä¹‰æ£€ç´¢ (Semantic Evidence) - å·²åšé˜²æ³„éœ²å¤„ç†
        semantic_info = "No database matches found."
        if vector_db:
            feat = semantic_extractor.extract(img_path)
            if feat is not None:
                sims = []
                feat = feat.detach().cpu()
                for db_name, db_feat in vector_db.items():
                    db_feat = db_feat.cpu()
                    if db_feat.shape[-1] == feat.shape[-1]:
                        sim = F.cosine_similarity(feat, db_feat).item()
                        sims.append((db_name, sim))
                
                # å–å‰3å
                top3 = sorted(sims, key=lambda x: x[1], reverse=True)[:3]
                
                # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šä¸å†å‘é€å…·ä½“æ–‡ä»¶åï¼Œåªå‘é€ç›¸ä¼¼åº¦åˆ†æ•°
                # é˜²æ­¢ GPT é€šè¿‡æ•°æ®åº“é‡Œçš„æ–‡ä»¶åï¼ˆå¦‚ "spoiled_meat_fake.jpg"ï¼‰ç›´æ¥çŒœåˆ°ç­”æ¡ˆ
                if top3:
                    top_score = top3[0][1]
                    if top_score > 0.8:
                        semantic_info = f"HIGH RISK: Found {len(top3)} similar cases in fake database. Top similarity score: {top_score:.2f} (Matches known fraud patterns)."
                    elif top_score > 0.6:
                        semantic_info = f"MEDIUM RISK: Moderate similarity to known fakes (Score: {top_score:.2f})."
                    else:
                        semantic_info = f"LOW RISK: Low similarity to database entries (Score: {top_score:.2f})."

        # 4. GPT ç”ŸæˆæŠ¥å‘Š
        report = brain.analyze_case({
            'filename': filename,
            'origin_path': img_path,
            'patch_path': patch_path,
            'verdict': verdict,
            'conf': conf,
            'semantic_info': semantic_info
        })

        # 5. å†™å…¥æ–‡ä»¶
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(f"--- Case: {filename} ---\n")
            f.write(f"Patch Map Found: {'YES' if patch_path else 'NO'}\n")
            f.write(report + "\n\n")
            f.write("="*50 + "\n\n")
        
        # ä¼‘æ¯ä¸€ä¸‹é˜²æ­¢ API é™æµ
        time.sleep(1)

    print(f"\nğŸ‰ All done! Reports saved to: {os.path.abspath(OUTPUT_FILE)}")

if __name__ == "__main__":
    main()