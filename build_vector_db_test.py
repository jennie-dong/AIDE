import torch
import os
import argparse
from tqdm import tqdm
import torch.nn.functional as F
import open_clip  # æ ¸å¿ƒä¾èµ–ï¼šç›´æ¥è°ƒç”¨ OpenCLIP
from PIL import Image
import glob

def build_database(args):
    # 1. æ£€æŸ¥è®¾å¤‡
    # ä¼˜å…ˆä½¿ç”¨ GPUï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ° CPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ [Test Script] Building Semantic DB on {device}...")

    # 2. åŠ è½½ OpenCLIP æ¨¡å‹ (ViT-B-32)
    # æ³¨æ„ï¼šå¿…é¡»ä¸ explainable_api.py ä¸­ä½¿ç”¨çš„æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼Œä¿è¯ç‰¹å¾ç©ºé—´å¯¹é½
    print("ğŸ”„ Loading OpenCLIP model (ViT-B-32)...")
    try:
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£… open_clip_torch: pip install open_clip_torch")
        return

    # 3. æ‰«æå›¾ç‰‡
    # æ”¯æŒé€’å½’æœç´¢æ‰€æœ‰å¸¸è§å›¾ç‰‡æ ¼å¼
    print(f"ğŸ“‚ Scanning images in: {args.data_path}")
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.webp']
    image_files = []
    
    for ext in image_extensions:
        # é€’å½’æŸ¥æ‰¾ (ä¾‹å¦‚ D:\...\**\*.jpg)
        found = glob.glob(os.path.join(args.data_path, "**", ext), recursive=True)
        image_files.extend(found)
    
    # è¿‡æ»¤é€»è¾‘ï¼ˆå¯é€‰ï¼‰ï¼šåªå¤„ç†æ–‡ä»¶åæˆ–è·¯å¾„åŒ…å« 'fake' çš„å›¾ç‰‡
    # å¦‚æœä½ çš„æ–‡ä»¶å¤¹é‡Œæ··æœ‰çœŸå›¾ï¼Œå»ºè®®å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
    # image_files = [f for f in image_files if "fake" in f.lower() or "1_fake" in f.lower()]

    if not image_files:
        print(f"âŒ Error: æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡! è¯·æ£€æŸ¥è·¯å¾„: {args.data_path}")
        return

    print(f"âœ… Found {len(image_files)} images. Start extracting features...")

    # 4. æå–ç‰¹å¾å¹¶æ„å»ºæ•°æ®åº“
    vector_db = {}
    success_count = 0
    
    with torch.no_grad():
        for img_path in tqdm(image_files, desc="Extracting"):
            try:
                # A. é¢„å¤„ç†
                image = Image.open(img_path).convert('RGB')
                image_tensor = preprocess(image).unsqueeze(0).to(device)
                
                # B. æå–ç‰¹å¾
                features = model.encode_image(image_tensor)
                
                # C. å½’ä¸€åŒ– (å…³é”®æ­¥éª¤ï¼šä½™å¼¦ç›¸ä¼¼åº¦è¦æ±‚å‘é‡å½’ä¸€åŒ–)
                norm_feat = F.normalize(features, dim=-1).cpu().squeeze(0)
                
                # D. å­˜å…¥å­—å…¸ {æ–‡ä»¶å: ç‰¹å¾å‘é‡}
                filename = os.path.basename(img_path)
                vector_db[filename] = norm_feat
                success_count += 1
                
            except Exception as e:
                print(f"\nâš ï¸ Skipping {os.path.basename(img_path)}: {e}")

    # 5. ä¿å­˜ç»“æœ
    if success_count > 0:
        os.makedirs(args.output_dir, exist_ok=True)
        save_path = os.path.join(args.output_dir, 'fake_image_vectors.pt')
        
        torch.save(vector_db, save_path)
        print(f"\nğŸ‰ Database generated successfully!")
        print(f"ğŸ’¾ Saved to: {os.path.abspath(save_path)}")
        print(f"Dg Total Vectors: {len(vector_db)}")
        print("ğŸ’¡ æç¤º: ç°åœ¨ä½ å¯ä»¥è¿è¡Œ explainable_api.py æ¥ä½¿ç”¨è¿™ä¸ªæ•°æ®åº“äº†ã€‚")
    else:
        print("\nâŒ æ²¡æœ‰ä»»ä½•ç‰¹å¾è¢«æå–ï¼Œæ•°æ®åº“æœªç”Ÿæˆã€‚")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Build Vector DB for Explainable Fake Detection (Test Version)")
    
    # å¿…å¡«ï¼šä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument('--data_path', type=str, required=True, help='Path to your image folder (e.g., D:/Datasets/Test/Fake)')
    
    # é€‰å¡«ï¼šä¿å­˜è·¯å¾„ (é»˜è®¤å½“å‰ç›®å½•)
    parser.add_argument('--output_dir', type=str, default='./', help='Output directory for .pt file')
    
    args = parser.parse_args()
    build_database(args)